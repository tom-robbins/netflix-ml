{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# HW3: Netflix Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this homework assignment, you will analyze the netflix prize data. The data consist of 100,480,50 movie ratings on a scale from 0 to 5 stars. The reveiws are distributed across 17,770 movies and 480,189. We have provided the training data as a sparse matrix where the row corresponds to the movie ID and the column corresponds to the user ID. A seperate file contains the title and year of release for each movie. The original, raw data consists of multiple lists of tuples; each list is a seperate movie and each tuple is User ID, Rating, and Rating Year. \n",
    "The original data can be downloaded here: https://archive.org/download/nf_prize_dataset.tar\n",
    "Further information about the netflix prize is available online: \n",
    "https://en.wikipedia.org/wiki/Netflix_Prize\n",
    "https://www.netflixprize.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse, multiprocessing, pickle, sys, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from scipy import spatial\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770, 3)\n"
     ]
    }
   ],
   "source": [
    "# This file consists of titles and release years associated with each ID\n",
    "movie_titles = pd.read_csv('movie_titles.txt', header = None, names = ['ID','Year','Name'])\n",
    "# print(movie_titles.head())\n",
    "print(movie_titles.shape)\n",
    "\n",
    "movie_by_id = {}\n",
    "for id, name, year in zip(movie_titles['ID'], movie_titles['Name'], movie_titles['Year']):\n",
    "    if not (np.isnan(year)):\n",
    "        year = str(int(year))\n",
    "    else:\n",
    "        year = 'NaN'\n",
    "    movie_by_id[id] = name + ' ' + '(' + year + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770, 28)\n"
     ]
    }
   ],
   "source": [
    "# import the movie genre data scraped using imdbpy\n",
    "movie_genres = pd.read_csv('../onehot_all_movie_genres.csv', header = 0)\n",
    "print(movie_genres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17771, 2649430)\n"
     ]
    }
   ],
   "source": [
    "# This file is a sparse matrix of movies by user, with each element a rating (1-5) or nonresponse (0)\n",
    "ratings_csr = sparse.load_npz('netflix_full_csr.npz')\n",
    "print(ratings_csr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To avoid memory overflow errors we have randomly subsampled the data. Some computers can handle the full dataset (e.g. 2017 Macbook Pro can perform SVD on the full dataset). Older computers likely need to subsample the data. You can consider using Princeton computing resources and clusters to perform more computationally expensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing users with no reviews:  (2649430, 17771)\n",
      "(480189, 17771)\n",
      "finished in 29.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# Filter the matris to remove rows with NO REVIEWS\n",
    "start = time.time()\n",
    "ratings_csc = ratings_csr.T\n",
    "print 'before removing users with no reviews: ', ratings_csc.shape\n",
    "non_zero_users_csc = ratings_csc[(ratings_csc.getnnz(axis=1) != 0)]\n",
    "print non_zero_users_csc.shape\n",
    "\n",
    "finish = time.time()\n",
    "print 'finished in %.2f seconds' % (finish - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A common methods for analyzing large datasets is dimension reduction. Here we perform a truncated SVD suited for sparse datasets and analyze which movies are associated with different latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "svd = TruncatedSVD(n_components = n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Z = svd.fit_transform(ratings_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "components = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22315634 0.02998073 0.01984643 0.01672574 0.01252159]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,n_components):\n",
    "    Z_sort = np.argsort(np.abs(Z[:,i]))\n",
    "    print('Component ' + str(i))\n",
    "    for j in range(1,10):\n",
    "        movie_index = Z_sort[-j]\n",
    "        movie_title = movie_titles[movie_titles['ID'] == movie_index]['Name']\n",
    "        movie_weight = Z[movie_index,i]\n",
    "        print(str(movie_title) + ': ' + str(movie_weight))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480189, 17771)\n"
     ]
    }
   ],
   "source": [
    "# construct a dictionary to store number of reviews per user\n",
    "print non_zero_users_csc.shape\n",
    "non_zero_users_csr = csr_matrix(non_zero_users_csc)\n",
    "\n",
    "reviews_by_user = {}\n",
    "for u in range(non_zero_users_csr.shape[0]):\n",
    "    reviews_by_user[u] = non_zero_users_csr[u].nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest amount of reviews per user: [17653, 17436, 16565, 15813, 14831, 9821, 9768, 9739, 9064, 8881]\n",
      "[55373, 70466, 442139, 301823, 383961, 265129, 297513, 238656, 472465, 350357]\n"
     ]
    }
   ],
   "source": [
    "s = sorted(reviews_by_user.keys(), key=lambda x: reviews_by_user[x], reverse=True)[:10]\n",
    "print 'highest amount of reviews per user:', [reviews_by_user[i] for i in s]\n",
    "print [i for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480189, 17771)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_users_csc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:5: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#revs\tavg.\tmovie\n",
      "681\t4.5389\tFruits Basket (2001)\n",
      "17292\t4.5426\tThe Simpsons: Season 5 (1993)\n",
      "92470\t4.5437\tStar Wars: Episode V: The Empire Strikes Back (1980)\n",
      "134284\t4.5451\tLord of the Rings: The Return of the King (2003)\n",
      "125\t4.5520\tLord of the Rings: The Return of the King: Extended Edition: Bonus Material (2003)\n",
      "1883\t4.5544\tInu-Yasha (2000)\n",
      "8426\t4.5813\tThe Simpsons: Season 6 (1994)\n",
      "6621\t4.5824\tArrested Development: Season 2 (2004)\n",
      "220\t4.5864\tGhost in the Shell: Stand Alone Complex: 2nd Gig (2005)\n",
      "1238\t4.5921\tVeronica Mars: Season 1 (2004)\n",
      "139660\t4.5934\tThe Shawshank Redemption: Special Edition (1994)\n",
      "89\t4.5955\tTenchi Muyo! Ryo Ohki (1995)\n",
      "25\t4.6000\tTrailer Park Boys: Season 4 (2003)\n",
      "75\t4.6000\tTrailer Park Boys: Season 3 (2003)\n",
      "1633\t4.6050\tFullmetal Alchemist (2004)\n",
      "1747\t4.6388\tBattlestar Galactica: Season 1 (2004)\n",
      "7249\t4.6710\tLost: Season 1 (2004)\n",
      "74912\t4.7026\tLord of the Rings: The Two Towers: Extended Edition (2002)\n",
      "73422\t4.7166\tThe Lord of the Rings: The Fellowship of the Ring: Extended Edition (2001)\n",
      "73335\t4.7233\tLord of the Rings: The Return of the King: Extended Edition (2003)\n"
     ]
    }
   ],
   "source": [
    "# count the number of reviews for each film and store in review_nums list\n",
    "review_nums = []\n",
    "for i in range(non_zero_users_csc.shape[1]):\n",
    "    num_reviews = non_zero_users_csc[:,i].nnz\n",
    "    review_nums.append((i, num_reviews, np.sum(non_zero_users_csc[:,i]) / num_reviews))\n",
    "\n",
    "# Print the top movies by number of reviews \n",
    "s = sorted(review_nums, key=lambda x: x[2])\n",
    "print '#revs\\tavg.\\tmovie'\n",
    "for movie_id, num, avg_review in s[-20:]:\n",
    "    print '%s\\t%0.4f\\t%s' % (num, avg_review, movie_by_id[movie_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# analyze the one guy who has seen 17,563 movies (WTF)\n",
    "hasnt_watched = []\n",
    "last = 0\n",
    "for i in sparse.find(non_zero_users_csr[55373])[1]:\n",
    "    if i - last > 1:\n",
    "        for j in range(1, i - last)[::-1]:\n",
    "            hasnt_watched.append(i - j)\n",
    "    last = i\n",
    "\n",
    "# sort by average star rating\n",
    "hasnt_watched = sorted(hasnt_watched, key=lambda x: review_nums[x][2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transpose back to (movie, user) orientation for effcient operations later\n",
    "ratings_small = non_zero_users_csc\n",
    "ratings_small = ratings_small.transpose()\n",
    "\n",
    "# Add in rows for each genre\n",
    "arr = np.array(movie_genres)\n",
    "arr = np.insert(arr, 0, 0, axis=0)  # add zero row to match the zero row in ratings_small\n",
    "csr_arr = csr_matrix(arr[:,1:])     # chop off the movie IDs and make it a csr matrix\n",
    "ratings_small_with_genres = sparse.hstack([ratings_small, csr_arr], format = 'csr')  # append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAIN REGRESSION CELL\n",
    "\n",
    "max_reviews = 1000                        # how many features are allowed in the regression\n",
    "movie_results_dict = {}                  # destination dict for metrics/results\n",
    "# top_reviewers, random_sample\n",
    "choosing_mechanism = 'random_sample'     # choose how to pick the features\n",
    "data = ratings_small_with_genres         # choose the matrix to regress on\n",
    "\n",
    "# pointers to the indexes of the genre features (not review data) appended at the end\n",
    "end_genre_index = ratings_small_with_genres.shape[1]\n",
    "num_genres = 27\n",
    "beg_genre_index = end_genre_index - num_genres\n",
    "genre_indexes = range(beg_genre_index, end_genre_index)\n",
    "\n",
    "# Loop over movie IDs, generating a model for each movie\n",
    "def run_regression(first_movie, last_movie, results_dict):\n",
    "    for movie_id in range(first_movie,last_movie):\n",
    "        # keep track of what movie you're on\n",
    "        num_reviews = data[movie_id,:beg_genre_index].count_nonzero()\n",
    "        movie_name = movie_by_id[movie_id]\n",
    "#         print 'Movie #%s, %s\\naverage rating: %.2f in %i reviews  | ' % (\n",
    "#             movie_id,\n",
    "#             movie_name[:40],\n",
    "#             np.sum(data[movie_id,:beg_genre_index]) / num_reviews,\n",
    "#             int(num_reviews)\n",
    "#         ),\n",
    "\n",
    "        ### PART I.\n",
    "        ### MAKE THE X, y TO FEED THE REGRESSION OUT OF THE REVIEW DATA\n",
    "        start = time.time()\n",
    "\n",
    "        # filter out the reviewers who havent seen the movie\n",
    "        user_mask = sparse.find(data[movie_id,:beg_genre_index])[1]\n",
    "\n",
    "        # if there are many reviewers, only take some (based on choosing_mechanism)\n",
    "        if (num_reviews > max_reviews):\n",
    "            if (choosing_mechanism == 'top_reviewers'):\n",
    "                user_mask = sorted(user_mask, key=lambda u: reviews_by_user[u])[:max_reviews]\n",
    "            elif (choosing_mechanism == 'random_sample'):\n",
    "                user_mask = user_mask[np.random.choice(len(user_mask), size=max_reviews)]\n",
    "\n",
    "        # Include the genre features\n",
    "        #user_mask = np.unique(np.hstack([genre_indexes, user_mask]))\n",
    "\n",
    "        # print 'regressing on %i features' % len(user_mask)\n",
    "\n",
    "        # apply mask to filter the users (axis 0) of the X matrix\n",
    "        ratings_filtered_by_user = data[:,user_mask]\n",
    "\n",
    "        # make mask to filter out only the reviews for the movie in question\n",
    "        movie_mask = np.ravel(np.full((ratings_filtered_by_user.shape[0], 1), True))\n",
    "        movie_mask[movie_id] = False\n",
    "\n",
    "        # generate X and y, training and testing data splitting\n",
    "        X = ratings_filtered_by_user[movie_mask]\n",
    "        y = ratings_filtered_by_user[movie_id]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.transpose().todense(), np.ravel(y.transpose().todense()), test_size=0.2, random_state=10)\n",
    "\n",
    "        finish = time.time()\n",
    "        data_time = (finish - start)\n",
    "\n",
    "        ### PART II\n",
    "        ### RUN THE REGRESSION ON THE FILM TO MAKE THE PREDICTOR\n",
    "        start = time.time()\n",
    "\n",
    "        # regr = LinearRegression()\n",
    "        # regr = RandomForestRegressor(n_estimators=10)\n",
    "        regr = Ridge(alpha=0.001)\n",
    "        # regr = AdaBoostRegressor()\n",
    "        regr.fit(X_train, y_train)\n",
    "        y_pred = regr.predict(X_test)\n",
    "\n",
    "        current_mse = mean_squared_error(y_test, y_pred)\n",
    "        current_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        finish = time.time()\n",
    "        regr_time = (finish - start)\n",
    "\n",
    "        # add data to results dictionary\n",
    "        results_dict[movie_id] = {\n",
    "            'name': movie_by_id[movie_id],\n",
    "            'regr_time': regr_time,\n",
    "            'data_time': data_time,\n",
    "            #'regr': regr,\n",
    "            'mse': mean_squared_error(y_test, y_pred),\n",
    "            'r2': r2_score(y_test, y_pred),\n",
    "        }\n",
    "#         print \"MSE: %.2f \\t\\t r2: %.2f \\t\\t time: %.2f ... (%.2f + %.2f)\" % (\n",
    "#             current_mse,\n",
    "#             current_r2,\n",
    "#             data_time + regr_time, data_time, regr_time\n",
    "#         )\n",
    "#         print\n",
    "        \n",
    "# run_regression(1, 50, movie_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "results_dict = manager.dict()\n",
    "processes = []\n",
    "\n",
    "movie_offset = 1\n",
    "num_movies = 10\n",
    "num_processes = 2\n",
    "\n",
    "movies_per_process = num_movies / num_processes\n",
    "\n",
    "for i in range(num_processes):\n",
    "    p = multiprocessing.Process(\n",
    "        target=run_regression, args=(\n",
    "            movie_offset + i*movies_per_process,\n",
    "            movie_offset + (i+1)*movies_per_process,\n",
    "            results_dict\n",
    "        )\n",
    "    )\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "0.641\tInspector Morse 31: Death Is Now My Neighbour (1997)\n",
      "0.602\tSick (1997)\n",
      "0.420\tMy Bloody Valentine (1981)\n",
      "0.405\tABC Primetime: Mel Gibson's The Passion of the Christ (2004)\n",
      "0.366\tIsle of Man TT 2004 Review (2004)\n",
      "0.340\tThe Rise and Fall of ECW (2004)\n",
      "0.296\tZatoichi's Conspiracy (1973)\n",
      "0.277\tLord of the Rings: The Return of the King: Extended Edition: Bonus Material (2003)\n",
      "0.275\tThe Bad and the Beautiful (1952)\n",
      "0.208\tCharacter (1997)\n",
      "\n",
      "average r2 value: -0.06\n",
      "\n",
      "MSE\n",
      "0.354\tLord of the Rings: The Return of the King: Extended Edition: Bonus Material (2003)\n",
      "0.466\tInspector Morse 31: Death Is Now My Neighbour (1997)\n",
      "0.605\tMy Bloody Valentine (1981)\n",
      "0.610\tHorror Vision (2000)\n",
      "0.669\tSick (1997)\n",
      "0.721\tCharacter (1997)\n",
      "0.793\tSearching for Paradise (2002)\n",
      "0.802\tABC Primetime: Mel Gibson's The Passion of the Christ (2004)\n",
      "0.856\tZatoichi's Conspiracy (1973)\n",
      "0.895\tImmortal Beloved (1994)\n",
      "\n",
      "average MSE value: 1.24\n"
     ]
    }
   ],
   "source": [
    "# cursory analytics\n",
    "def average_nested_dict_key(d, k):\n",
    "    sum = np.sum([d[i][k] for i in d.keys()])\n",
    "    return sum / len(d.keys())\n",
    "        \n",
    "\n",
    "ks = movie_results_dict.keys()\n",
    "# sort by r2 value\n",
    "print 'r2'\n",
    "s = sorted(ks, key=lambda x: movie_results_dict[x]['r2'], reverse=True)\n",
    "for i in s[:10]:\n",
    "    print '%.3f\\t%s' % (movie_results_dict[i]['r2'], movie_results_dict[i]['name'])\n",
    "\n",
    "print\n",
    "print 'average r2 value: %.2f' % average_nested_dict_key(movie_results_dict, 'r2')\n",
    "    \n",
    "# sort by mse\n",
    "print '\\nMSE'\n",
    "s = sorted(ks, key=lambda x: movie_results_dict[x]['mse'])\n",
    "for i in s[:10]:\n",
    "    print '%.3f\\t%s' % (movie_results_dict[i]['mse'], movie_results_dict[i]['name'])\n",
    "    \n",
    "print\n",
    "print 'average MSE value: %.2f' % average_nested_dict_key(movie_results_dict, 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scratch space for pickling data structures for safe keeping\n",
    "import pickle\n",
    "with open('movie_results_dict_1000_1500.pickle', 'wb') as f:\n",
    "    pickle.dump(movie_results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with genre\n",
      "average r2 value: -0.48\n",
      "average mse value: 2.78\n",
      "without genre\n",
      "average r2 value: 0.03\n",
      "average mse value: 1.37\n"
     ]
    }
   ],
   "source": [
    "# scratch space for comparing runs of regressions\n",
    "\n",
    "print 'with genre'\n",
    "print 'average r2 value: %.2f' % average_nested_dict_key(with_genre, 'r2')\n",
    "print 'average mse value: %.2f' % average_nested_dict_key(with_genre, 'mse')\n",
    "\n",
    "print 'without genre'\n",
    "print 'average r2 value: %.2f' % average_nested_dict_key(without_genre, 'r2')\n",
    "print 'average mse value: %.2f' % average_nested_dict_key(without_genre, 'mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "without_genre = movie_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
